response = query_engine.query("Расскажи про должностные обзяности грузчика ?.")
#print(response)
"""Ответственность грузчика:

* Невыполнение и/или несвоевременное, нехалатное выполнение своих должностных обязанности;
* Нарушение действующих инструкций, приказов и распоряджений по сохранению коммерческой тайны и секретной информации;
* Нарушение правил внутреннего трудового распорядка и трудовой дисциплины;
* Нарушение правил техники безопасности и противопожерной безопасности. """

# Улучшение RAG
# Ипользую метод ранжирования LLMRerank
# Импортируем необходимые классы из библиотеки llama_index
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.postprocessor import LLMRerank  # модуль реранжирования на базе LLM

# Создаем объект поискового движка из индекса
query_engine = index.as_query_engine(
    similarity_top_k=5,  # Указываем, что хотим получить топ-5 наиболее похожих результатов
    node_postprocessors=[
        LLMRerank(  # Используем LLMRerank для реранжирования результатов
            choice_batch_size=5,  # Размер батча для обработки
            top_n=2,  # Оставляем только топ-2 результата после реранжирования
        )
    ],
)

# Выполняем запрос к поисковому движку
response = query_engine.query(
    "Расскажи про должностные обзяности грузчика ? Опираясь на контекст документа, если сомвневаешься в ответе, то ничего не придумывай")

# Выводим ответ на запрос
print(response)

"""Резульата не принесло:
Ответственность грузчика:

1. За невыполнение и/или несвоейвременное, халатное выполнение своих должностних обязанностей.
2. За несоблюдение действующих инструкции, приказов и распоряжений по сохранении коммерческой тайны и конфиденциальной информацию.
3. За нарушение правил внутреннего трудоowego распорядка, трудовой дисциплины,правил техники безопасности и противопожарнной безопасности."""

# Уменьшаем размер чанков и настраиваем перекрытие
Settings.chunk_size = 256  # уменьшаем с 512 до 256
Settings.chunk_overlap = 64  # добавляем перекрытие

# Пробуем другую модель эмбеддингов
embed_model = LangchainEmbedding(
    HuggingFaceEmbeddings(model_name="intfloat/e5-large-v2")
)

# Создаем query_engine без LLMRerank
query_engine = index.as_query_engine(
    similarity_top_k=5
)

response = query_engine.query(
    "Расскажи про должностные обязанности грузчика. Используй только информацию из документа. Если какой-то пункт не описан в документе, не придумывай его. Структурируй ответ в виде нумерованного списка."
)
print(response)

""" Результат:
1. Осуществляет погрузку и разгрузку.
2. Производит очистку подвижного состаva после произведенной выгрузки груза.
3. Чистит и смазывает обслуживаемые подвижные составы и средства транспортировки.
4. Вносит на рассмотрение руководства предложения по совершенованию работы, связанной с предусмотренным настоящей инструкции обязанностями.
5. Требует от руководства предприятия обеспечения организационнотехнических условий и оформления установледных документов, необходимых для исполнение должностных обязанностей.
Согласно данным из файла NeyroWork.txt"""

"""Вывод по улучшению RAG
Удалось победить галяюцинации путями:

Улучшенный запрос с инструкциями
Настройкой параметров индексации - уменишил чанки, добавил перекрытие
Добавление другой модели эмбедингов, что я считаю и повлияло на улучшения ответа и победой на галюцинациями
Как видно выше, LLMRank нам ничего не дал, хотя бываил даже ошибки при выполнении запроса, а вывод запрсоа и вовсе не про то, что спрашивал, так что принял решение отключить LLMRank вовсе."""
